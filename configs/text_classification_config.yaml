---
##############################################################################################
#Default parameter file for tensor evolution
##############################################################################################


#This is a list of lists (i.e. nested list) representing the input shape(s).
#If you have multiple inputs then the outermost list should have multiple entries
#e.g. input_shapes[[5,5], [3]] represents two inputs, one shape (5,5), and one shape (3,)
input_shapes: [["None"]]
input_dtype: "None"
#List of output shapes
num_outputs: [1]

direction: max #is fitness maximized or minimized. use "min" for minimized

####
# Ray remote settings
####
remote: False #run using Ray remote actors
remote_actors: 4 #number of actors to use for remote execution
remote_actor_cpus: 1
remote_actor_gpus: 0


####
# Backends
####
backend: "tf" #library used to build and train neural networks


####
# Layer Controls
####

#Valid nodes to use in genomes
valid_node_types: ["DenseNode", "ReluNode", "BatchNormNode",
                   "MaxPool2DNode", "Conv2dNode", "AdditionNode", "FlattenNode",
                   "DropoutNode", "Conv3dNode", "MaxPool3DNode", "LstmNode",
                   "GlobalAveragePooling1DNode"]

#Controls max size of dense layers. Dense layer sizes are always a power of 2.
dense_max_power_two: 8
#Conrols max number of filters on Conv2D, 3D layers. Number of filters is always a power of 2
max_conv_power: 5
lstm_power_two: 4

#Valid kernel sizes for Conv2D layers
conv2d_kernel_sizes: [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]
#Valid sizes for max pooling 2D layers
max_pooling_size2D: [[2, 2], [3, 3]]
#Valid kernel sizes for Conv3D layers
conv3d_kernel_sizes: [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5]]
#Valid sizes for max pooling 3D layers
max_pooling_size3D: [[2, 2, 2], [3, 3, 3]]

#If True then layers will attempt to save their trained weights across generations
global_cache_training: True


####
# Evolution Controls
####
cx: 0.4 #cross over prob
m_insert: 0.3 #prob to insert a new layer into a genome
m_del: 0.2 #prob to delete a layer from a genome
m_mut: 0.2 #prob to mutate an existing layer in a genome
pop_size: 50 #population size
fitness_t_size: 7 #fitness tournament size
complexity_t_size: 2 #complexity tournament size
prob_sel_least_complex: 0.8 #prob to select least complex network in a complexity tournament
ngen: 30 #generations to run evolution for
max_network_size: 10 #maximum size of the generated neural networks, doesn't include inputs, outputs, or preprocessing


####
# NN Training Controls
####
opt: {optimizer: 'adam', learning_rate: 0.001} #optimizer
learning_rates: [0.0001, 0.001, 0.01, 0.1]
regulizer_factor: [0.0001, 0.001, 0.01, 0.1] #factors for kernel regularization
regularizer_types: ['L1', 'L2', 'L1L2', 'None'] #kernel regularization
                                                #only use "None" for no regularization.
                                                #Omit "None" if you want every node that
                                                #can regularize to do so
loss: 'BinaryCrossentropy' #loss function
metrics: ['accuracy'] #metrics for neural network model
max_fit_epochs: 6 #max number of epochs to train a genome for
early_stopping: {use_early: True, monitor: 'loss', patience: 2 ,min_delta: 0.02}
verbose: [0,1] #train verbose, test verbose
batch_size: 'None' # batch sized used for the NN training fit method


####
# Hyperparam evolution
####
evolve_hyperparams: True
hyper_mut: 0.1 # prob to mutate hyperparms
hyper_cx: 0.2 # prob of hyperparam crossover
hyper_cx_uniform_prob: 0.2 # cross over is uniform type, this is prob that any given hyper param is swapped
mutatable_keys: ['cx', 'm_mut', 'm_insert', 'm_del', 'hyper_cx',
                 'hyper_mut', 'learning_rate', 'opt'] # which hyperparams are valid to mutate


####
# Save settings
####
save_pop_every: 5 # save population to disk every n generations
save_pop_filepath: 'pop.txt' # path to save population
preprocessing_save_path: ['save'] # path to save preprocessing layers. needs to be a directory. One for each set of preprocessing layers
...