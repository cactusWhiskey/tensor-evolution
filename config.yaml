---
##############################################################################################
#Default parameter file for tensor evolution
##############################################################################################


#This is a list of lists (i.e. nested list) representing the input shape(s).
#If you have multiple inputs then the outermost list should have multiple entries
#e.g. input_shapes[[5,5], [3]] represents two inputs, one shape (5,5), and one shape (3,)
input_shapes: [[28, 28]]
#List of output shapes
num_outputs: [10]

remote: False #run using Ray remote actors
remote_actors: 1 #number of actors to use for remote execution

backend: "tf" #library used to build and train neural networks

#Valid nodes to use in genomes
valid_node_types: ["Dense", "ReLU", "BatchNormalization", "MaxPooling2D", "Conv2D", "add"]

#Controls max size of dense layers. Dense layer sizes are always a power of 2.
dense_max_power_two: 6
#Conrols max number of filters on Conv2D layers. Number of filters is always a power of 2
max_conv2d_power: 6

#Valid kernel sizes for Conv2D layers
conv2d_kernel_sizes: [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]
#Valid sizes for max pooling 2D layers
max_pooling_size: [[2, 2], [3, 3]]

#If True then layers will attempt to save their trained weights across generations
global_cache_training: True

cx: 0.4 #cross over prob
m_insert: 0.3 #prob to insert a new layer into a genome
m_del: 0.1 #prob to delete a layer from a genome
m_mut: 0.2 #prob to mutate an existing layer in a genome
pop_size: 10 #population size
t_size: 3 #tournament size
ngen: 100 #generations to run evolution for
complexity_penalty: 0.002 #penelty to fitness based on genome size. Calculated as penalty*genome length
opt: {optimizer: 'adam', learning_rate: 0.001} #optimizer
loss: 'SparseCategoricalCrossentropy' #loss function
metrics: ['accuracy'] #metrics for neural network model
max_fit_epochs: 2 #max number of epochs to train a genome for
early_stopping: {use_early: True, monitor: 'loss', patience: 2 ,min_delta: 0.01}
...