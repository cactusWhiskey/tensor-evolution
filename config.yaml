---
##############################################################################################
#Default parameter file for tensor evolution
##############################################################################################


#This is a list of lists (i.e. nested list) representing the input shape(s).
#If you have multiple inputs then the outermost list should have multiple entries
#e.g. input_shapes[[5,5], [3]] represents two inputs, one shape (5,5), and one shape (3,)
input_shapes: [[28, 28]]
#List of output shapes
num_outputs: [10]


####
# Ray remote settings
####
remote: False #run using Ray remote actors
remote_actors: 3 #number of actors to use for remote execution
remote_actor_cpus: 1


####
# Backends
####
backend: "tf" #library used to build and train neural networks


####
# Layer Controls
####
#Valid nodes to use in genomes
valid_node_types: ["DenseNode", "ReluNode", "BatchNormNode",
                   "MaxPool2DNode", "Conv2dNode", "AdditionNode", "FlattenNode"]

#Controls max size of dense layers. Dense layer sizes are always a power of 2.
dense_max_power_two: 6
#Conrols max number of filters on Conv2D layers. Number of filters is always a power of 2
max_conv2d_power: 6

#Valid kernel sizes for Conv2D layers
conv2d_kernel_sizes: [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]
#Valid sizes for max pooling 2D layers
max_pooling_size: [[2, 2], [3, 3]]

#If True then layers will attempt to save their trained weights across generations
global_cache_training: True


####
# Evolution Controls
####
cx: 0.4 #cross over prob
m_insert: 0.3 #prob to insert a new layer into a genome
m_del: 0.1 #prob to delete a layer from a genome
m_mut: 0.2 #prob to mutate an existing layer in a genome
pop_size: 30 #population size
t_size: 3 #tournament size
ngen: 30 #generations to run evolution for
complexity_penalty: 0.002 #penelty to fitness based on genome size. Calculated as penalty*genome length


####
# NN Training Controls
####
opt: {optimizer: 'adam', learning_rate: 0.001} #optimizer
learning_rates: [0.0001, 0.001, 0.01, 0.1]
loss: 'SparseCategoricalCrossentropy' #loss function
metrics: ['accuracy'] #metrics for neural network model
max_fit_epochs: 5 #max number of epochs to train a genome for
early_stopping: {use_early: True, monitor: 'loss', patience: 2 ,min_delta: 0.02}
verbose: 0


####
# Hyperparam evolution
####
evolve_hyperparams: True
hyper_mut: 0.1 # prob to mutate hyperparms
hyper_cx: 0.2 # prob of hyperparam crossover
hyper_cx_uniform_prob: 0.2 # cross over is uniform type, his is prob that any given hyper param is swapped
mutatable_keys: ['cx', 'm_mut', 'm_insert', 'm_del', 'hyper_cx',
                 'hyper_mut', 'learning_rate', 'opt']


####
# Save settings
####
save_pop_every: 5 # save population to disk every n generations
save_pop_filepath: 'C:\Users\arjud\PycharmProjects\pythonProject\pop_save.txt'
...